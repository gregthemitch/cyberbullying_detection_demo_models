{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNs9J73f6OOfnlkRa/xXw8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregthemitch/cyberbullying_detection_demo_models/blob/main/Cyberbullying_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Kaggle Dataset"
      ],
      "metadata": {
        "id": "zyd2Pvdjs7Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d andrewmvd/cyberbullying-classification -f cyberbullying_tweets.csv -p ./project/data\n",
        "# Unzip csv from zip file\n",
        "!unzip -j \"./project/data/cyberbullying_tweets.csv.zip\" \"*.csv\" -d \"./project/data\"\n",
        "# Remove zip file\n",
        "!rm ./project/data/cyberbullying_tweets.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHCOfAahsN1L",
        "outputId": "92e7abb6-5170-4ffc-b71c-ece528865963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "Downloading cyberbullying_tweets.csv.zip to ./project/data\n",
            "  0% 0.00/2.82M [00:00<?, ?B/s]\n",
            "100% 2.82M/2.82M [00:00<00:00, 229MB/s]\n",
            "Archive:  ./project/data/cyberbullying_tweets.csv.zip\n",
            "replace ./project/data/cyberbullying_tweets.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ./project/data/cyberbullying_tweets.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Processing and Feature Selection"
      ],
      "metadata": {
        "id": "k0M6XTwfs_vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "import gensim\n",
        "import gensim.downloader"
      ],
      "metadata": {
        "id": "dZJFcY2isvxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb3uPSmnr8x2"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"project/data/cyberbullying_tweets.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in GloVe vectors for twitter\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')"
      ],
      "metadata": {
        "id": "G04vItPetMhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords from nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7fTGMlety4h",
        "outputId": "89350484-6b1b-4fcb-aaca-9ee14ca6cd51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import scipy\n",
        "import gensim\n",
        "import nltk\n",
        "import sys\n",
        "import joblib\n",
        "\n",
        "print(\"Python version: \", sys.version)\n",
        "\n",
        "print(\"numpy version: \", np.__version__)\n",
        "print(\"pandas version: \", pd.__version__)\n",
        "print(\"nltk version: \", nltk.__version__)\n",
        "print(\"sklearn version: \", sklearn.__version__)\n",
        "print(\"scipy version: \", scipy.__version__)\n",
        "print(\"gensim version: \", gensim.__version__)\n",
        "print(\"joblib version: \", joblib.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvfRyk9vjy1R",
        "outputId": "fc2d2595-25f7-416a-e6c3-41320bb630af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version:  3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
            "numpy version:  1.26.4\n",
            "pandas version:  2.2.2\n",
            "nltk version:  3.9.1\n",
            "sklearn version:  1.6.0\n",
            "scipy version:  1.13.1\n",
            "gensim version:  4.3.3\n",
            "joblib version:  1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build pipeline"
      ],
      "metadata": {
        "id": "wKrWea2z4ks4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy import sparse\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import hinge_loss, classification_report, confusion_matrix\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "\n",
        "# Custom transformer to clean data\n",
        "class CleanText(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "      \"\"\"\n",
        "      Function to clean an indiviudal string, removing stop words and punctuation.\n",
        "\n",
        "      Returns (str): The cleaned string.\n",
        "      \"\"\"\n",
        "      # Get relevant english stop words from nltk\n",
        "      STOP_WORDS = set(nltk.corpus.stopwords.words('english'))\n",
        "      # \\w is for words\n",
        "      WORD = re.compile(r'\\w+')\n",
        "\n",
        "      # Remove usernames (strings following @)\n",
        "      clean_text = re.sub(r\"@[a-z]+\", \"\", text.lower())\n",
        "      # Remove punctuation\n",
        "      clean_text = clean_text.translate(\n",
        "          str.maketrans('', '', string.punctuation)\n",
        "      ).replace(\"“\", \"\").replace(\"”\", \"\")\n",
        "\n",
        "      words = WORD.findall(clean_text)\n",
        "      return np.array(\n",
        "          \" \".join([word for word in words if word not in STOP_WORDS])\n",
        "      )\n",
        "\n",
        "    def _clean_data(self, X):\n",
        "      \"\"\"\n",
        "      Takes in an array of strings and cleans each string.\n",
        "\n",
        "      Returns (np.array): An array of cleaned strings.\n",
        "      \"\"\"\n",
        "      cleaned = np.array([],  dtype=object)\n",
        "\n",
        "      for i, sentence in enumerate(X):\n",
        "        cleaned = np.append(cleaned, self._clean_text(sentence))\n",
        "\n",
        "      return cleaned\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self._clean_data(X)\n",
        "\n",
        "# Transformer to numerically embed sentences\n",
        "class SentenceEmbedder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, word_embedding_model: gensim.models.KeyedVectors):\n",
        "      self.word_embedding_model = word_embedding_model\n",
        "\n",
        "    def _word_embedding(self, X):\n",
        "      \"\"\"\n",
        "      Takes in an array of strings and converts each string into a vector. The\n",
        "      method involves getting the average of the word vectors for each sentence.\n",
        "\n",
        "      Returns (np.array): A 2D array (n x length of word vectors).\n",
        "      \"\"\"\n",
        "\n",
        "      # Get sentence embeddings using the average of its words' embeddings\n",
        "      sentence_embeddings = []\n",
        "      for i, sentence in enumerate(X):\n",
        "          if sentence:\n",
        "              sentence_vec = self.word_embedding_model.get_mean_vector(\n",
        "                  sentence.split(\" \"), ignore_missing=True).reshape(1, -1)\n",
        "\n",
        "              if not sentence_vec.any():\n",
        "                  raise ValueError(\n",
        "                      \"Could not create sentence embedding for sentence: \"\n",
        "                      f\"{sentence}\"\n",
        "                  )\n",
        "\n",
        "              sentence_embeddings.append(sentence_vec)\n",
        "\n",
        "          else:\n",
        "            raise ValueError(\n",
        "                \"Sentence is empty. X and Y will have mismatched dimensions\"\n",
        "            )\n",
        "\n",
        "      return np.concatenate(sentence_embeddings, axis=0)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "      self.tfidf = TfidfVectorizer()\n",
        "      self.tfidf.fit(X)\n",
        "\n",
        "      return self\n",
        "\n",
        "    def transform(self, X):\n",
        "      sent_tfidf = self.tfidf.transform(X)\n",
        "      sent_vec = self._word_embedding(X)\n",
        "\n",
        "      return sparse.hstack([sparse.csr_array(sent_vec), sent_tfidf])\n",
        "\n",
        "# Transformer for truncated SVD\n",
        "# This is a holdover from the class project's manual implmentation\n",
        "class TruncatedSVD(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, n_components):\n",
        "    self.n_components = n_components\n",
        "    self.Vt = None\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Reduces dimensions using truncated SVD for sparse matrices.\n",
        "    \"\"\"\n",
        "    assert self.n_components <= X.shape[1], \\\n",
        "      \"Number of components is greater than data's number of features\"\n",
        "\n",
        "    U, S, Vt = svds(X, k=self.n_components)\n",
        "    self.Vt = Vt\n",
        "\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    # print(f\"Reducing dimensions to {self.n_components} using truncated SVD...\")\n",
        "    return X @ self.Vt.T\n"
      ],
      "metadata": {
        "id": "YCTcr4NkBMvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def report_leakage(X_train, X_test):\n",
        "  leaks = {}\n",
        "\n",
        "  for i, row in enumerate(X_train):\n",
        "    for j, test_row in enumerate(X_test):\n",
        "      if row == test_row:\n",
        "        # raise Exception(f\"row {i} in training data is in testing data in row {j}\")\n",
        "        leaks[row] = [i, j]\n",
        "\n",
        "\n",
        "  return leaks\n",
        "\n",
        "leaks = report_leakage(X_train, X_test)\n",
        "print(len(leaks))"
      ],
      "metadata": {
        "id": "voYOKSRdxPi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bullying_category(df):\n",
        "    \"\"\"\n",
        "    Split data into datasets by cyberbullying category, with no cyberbullying\n",
        "    and a cyberbullying category\n",
        "    \"\"\"\n",
        "    datasets = {\n",
        "        'other_cyberbullying': None,\n",
        "        'religion': None,\n",
        "        'gender': None,\n",
        "        'ethnicity': None,\n",
        "        'age': None\n",
        "    }\n",
        "\n",
        "    for key in datasets:\n",
        "        filtered_df = df[df[\"cyberbullying_type\"].isin(['not_cyberbullying', key])].copy()\n",
        "        filtered_df[\"encoded_y\"] = (filtered_df[\"cyberbullying_type\"] == key).astype(int)\n",
        "        filtered_df.loc[filtered_df[\"encoded_y\"] == 0, \"encoded_y\"] = -1\n",
        "\n",
        "        datasets[key] = filtered_df.reset_index(drop=True)\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "def preprocess(df, embedding_model):\n",
        "  idx = []\n",
        "\n",
        "  for i, text in enumerate(df[\"tweet_text\"]):\n",
        "    # Get relevant english stop words from nltk\n",
        "    STOP_WORDS = set(nltk.corpus.stopwords.words('english'))\n",
        "    # \\w is for words\n",
        "    WORD = re.compile(r'\\w+')\n",
        "\n",
        "    # Remove usernames (strings following @)\n",
        "    clean_text = re.sub(r\"@[a-z]+\", \"\", text.lower())\n",
        "    # Remove punctuation\n",
        "    clean_text = clean_text.translate(\n",
        "        str.maketrans('', '', string.punctuation)\n",
        "    ).replace(\"“\", \"\").replace(\"”\", \"\")\n",
        "\n",
        "    words = WORD.findall(clean_text)\n",
        "    if [word for word in words if (word not in STOP_WORDS) and (word in embedding_model)]:\n",
        "      idx.append(i)\n",
        "\n",
        "  return df.loc[idx, :].reset_index(drop=True)\n",
        "\n",
        "\n",
        "def split_data(df, train_size):\n",
        "  idx = np.arange(df.shape[0])\n",
        "  np.random.shuffle(idx)\n",
        "\n",
        "  train_size = int(train_size * df.shape[0])\n",
        "  test_size = df.shape[0] - train_size\n",
        "\n",
        "  train_idx = idx[:train_size]\n",
        "  test_idx = idx[train_size:]\n",
        "\n",
        "  return df.loc[train_idx], df.loc[test_idx]\n",
        "\n",
        "\n",
        "def get_data(df, train_size, embedding_model):\n",
        "  data_by_cat = bullying_category(data)\n",
        "  cleaned_data = {key: preprocess(value, embedding_model)\n",
        "    for key, value in data_by_cat.items()}\n",
        "\n",
        "  final_data = {}\n",
        "  for key, value in cleaned_data.items():\n",
        "    final_data[key] = split_data(value, train_size)\n",
        "\n",
        "  return final_data\n"
      ],
      "metadata": {
        "id": "TezEVIVdf7g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "def prepare_data_for_pipeline(data_dictionary, category):\n",
        "  X_train, X_test = data_dictionary[category][0][\"tweet_text\"],\\\n",
        "    data_dictionary[category][1][\"tweet_text\"]\n",
        "\n",
        "  y_train, y_test = data_dictionary[category][0][\"encoded_y\"],\\\n",
        "    data_dictionary[category][1][\"encoded_y\"]\n",
        "\n",
        "  return X_train.to_numpy(), X_test.to_numpy(), y_train.to_numpy(), y_test.to_numpy()\n",
        "\n",
        "\n",
        "def save_models(data_dictionary):\n",
        "  \"\"\"\n",
        "  Create pipelines for each category of cyberbullying and save the models/pipelines.\n",
        "  Code also downloads the model.\n",
        "  \"\"\"\n",
        "  for category in data_dictionary:\n",
        "    X_train, X_test, y_train, y_test = prepare_data_for_pipeline(data_dictionary, category)\n",
        "\n",
        "    pipe = Pipeline([\n",
        "      ('cleaning', CleanText()),\n",
        "      ('embedding', SentenceEmbedder(glove_vectors)),\n",
        "      ('svd', TruncatedSVD(500)),\n",
        "      ('svc', SVC(kernel='poly'))\n",
        "    ])\n",
        "\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    with open(f'{category}.z', \"wb\") as f:\n",
        "      dump(pipe, f, protocol=5)\n",
        "\n",
        "    # files.download(f'{category}.z')\n",
        "\n",
        "# Using 80% of the data to train the model\n",
        "data_dict = get_data(data, .8, glove_vectors)\n",
        "save_models(data_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "wZZjqKoWRNTt",
        "outputId": "7bca1788-1b77-4839-ab5a-ed02db3cbc38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8033476a-e667-492c-a33c-d84400c2ff48\", \"other_cyberbullying.z\", 277366812)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d1dc70c7-d458-47be-a174-d3e592338f60\", \"religion.z\", 277388892)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_77285377-a549-4c29-8fc5-af2e2cd050ab\", \"gender.z\", 269319228)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5a937f55-d3f3-4e63-a148-5effb2669ab5\", \"ethnicity.z\", 262308700)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ad9ebab7-d7c8-4c87-b3f0-5d89f57536aa\", \"age.z\", 262179436)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.score(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "361X_HYI_yas",
        "outputId": "23ca89fe-88df-44b8-b71f-afe2b5e6dbe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reducing dimensions to 10 using truncated SVD...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9304320203303685"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC2_7Lrw_jEo",
        "outputId": "09a84108-25dc-4c38-e7d0-03833a74abe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reducing dimensions to 10 using truncated SVD...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9307716735471578"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.predict(np.array([\"motherfuckers\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzzMGzyutcHE",
        "outputId": "888e8ba1-bcde-4324-9d24-dc5935709516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reducing dimensions to 500 using truncated SVD...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "with open(\"ethnicity.z\", \"rb\") as f:\n",
        "    test = load(f)"
      ],
      "metadata": {
        "id": "WE-3WOBTrRa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Attempt to use ONNX\n",
        "# !pip install skl2onnx\n",
        "# !pip install onnxruntime\n",
        "\n",
        "# from skl2onnx import to_onnx\n",
        "# import onnxruntime as rt\n",
        "\n",
        "# class Cyberbullying_Detector:\n",
        "#   def __init__(self, category, path=\"\") -> None:\n",
        "#     self.category = category\n",
        "#     self.Vt = None\n",
        "\n",
        "#     self._trained = False\n",
        "#     self._onnx_loaded = False\n",
        "\n",
        "#   def train(self, df, category_name, n_components, train_size, valid_size, test_size, **kwargs):\n",
        "\n",
        "#     self._trained = True\n",
        "#     pass\n",
        "\n",
        "#   def predict(self, text):\n",
        "#     pass\n",
        "\n",
        "#   def save(self, path):\n",
        "#     # Convert into ONNX format.\n",
        "#     onx = to_onnx(clr, X[:1])\n",
        "\n",
        "#     with open(f\"{path}cb_{self.category}.onnx\", \"wb\") as f:\n",
        "#         f.write(onx.SerializeToString())\n",
        "\n",
        "#   def load(self, path)\n",
        "#     sess = rt.InferenceSession(\"rf_iris.onnx\", providers=[\"CPUExecutionProvider\"])\n",
        "#     input_name = sess.get_inputs()[0].name\n",
        "#     label_name = sess.get_outputs()[0].name\n",
        "#     pred_onx = sess.run([label_name], {input_name: X_test.astype(np.float32)})[0]"
      ],
      "metadata": {
        "id": "8MSftFrQfwqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install skl2onnx\n",
        "# !pip install onnxruntime\n",
        "\n",
        "# import numpy as np\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# iris = load_iris()\n",
        "# X, y = iris.data, iris.target\n",
        "# X = X.astype(np.float32)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "# clr = RandomForestClassifier()\n",
        "# clr.fit(X_train, y_train)\n",
        "\n",
        "# # Convert into ONNX format.\n",
        "# from skl2onnx import to_onnx\n",
        "\n",
        "# onx = to_onnx(clr, X[:1])\n",
        "# with open(\"rf_iris.onnx\", \"wb\") as f:\n",
        "#     f.write(onx.SerializeToString())\n",
        "\n",
        "# # Compute the prediction with onnxruntime.\n",
        "# import onnxruntime as rt\n",
        "\n",
        "# sess = rt.InferenceSession(\"rf_iris.onnx\", providers=[\"CPUExecutionProvider\"])\n",
        "# input_name = sess.get_inputs()[0].name\n",
        "# label_name = sess.get_outputs()[0].name\n",
        "# pred_onx = sess.run([label_name], {input_name: X_test.astype(np.float32)})[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi2DTdT3mBDC",
        "outputId": "89f8582d-f596-420e-c0e3-589305714a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: skl2onnx in /usr/local/lib/python3.10/dist-packages (1.18.0)\n",
            "Requirement already satisfied: onnx>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from skl2onnx) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.10/dist-packages (from skl2onnx) (1.6.0)\n",
            "Requirement already satisfied: onnxconverter-common>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from skl2onnx) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.2.1->skl2onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.2.1->skl2onnx) (3.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxconverter-common>=1.7.0->skl2onnx) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1->skl2onnx) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1->skl2onnx) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1->skl2onnx) (3.5.0)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.20.1\n"
          ]
        }
      ]
    }
  ]
}